import os
from dotenv import load_dotenv
from fastapi import FastAPI, Request, Form
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Load environment variables from .env file
load_dotenv()

# -- App Setup --
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# -- Document Loading & Chunking (Semantic with Overlap) --
loader = DirectoryLoader(
    "SAMA_rulebook_dataset",    # Relative path to your dataset folder
    glob="*.pdf",               # Loads only PDF files
    loader_cls=PyPDFLoader,     # Use PyPDFLoader for PDF compatibility
    show_progress=True,
    use_multithreading=True
)
raw_docs = loader.load()

#print(len(raw_docs))

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=100,
    separators=["\n\n", "\n", ".", "ØŸ", "!", " "] # The **separators** parameter in `RecursiveCharacterTextSplitter` controls **where**The separators parameter in RecursiveCharacterTextSplitter controls where the text will be split to create chunks.
)
docs = text_splitter.split_documents(raw_docs) # docs is a list of document chunks.

# -- Embedding Vector Store (ChromaDB) --

embedding = OpenAIEmbeddings(
    model = 'text-embedding-3-large',
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

vectordb = Chroma.from_documents(
    docs,
    embedding=embedding,
    collection_metadata={"hnsw:space": "cosine"}
)

# -- Prompt Engineering --
prompt_template = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± ÙÙŠ Ø£Ù†Ø¸Ù…Ø© ÙˆÙ‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.
Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ§ÙÙ‡Ù… Ù…Ø­ØªÙˆØ§Ù‡Ø§ØŒ Ø«Ù… Ø¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ ÙÙ‡Ù…ØªÙ‡ Ù…Ù†Ù‡Ø§.
Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø© Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù‚Ù„ "Ù„Ø§ Ø£Ø¹Ø±Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©".
Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù…Ù‚ØªØ·Ù Ù…Ø¹ÙŠÙ‘Ù†ØŒ Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙƒÙ…ØµØ¯Ø± Ø¥Ù† Ø£Ù…ÙƒÙ†.

Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""

prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])


retriever = vectordb.as_retriever()
llm = ChatOpenAI(model="gpt-4-turbo", openai_api_key=os.getenv("OPENAI_API_KEY"))

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# -- FastAPI Endpoint --
@app.get("/", response_class=None)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request, "result": None})

@app.post("/", response_class=None)
async def ask_question(request: Request, question: str = Form(...)):
    if not question.strip():
        return templates.TemplateResponse("index.html", {"request": request, "result": None, "question": question})

    result = qa_chain({"query": question})
    print("\n\n=========== Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù…Ù† ChromaDB ===========\n")
    for i, doc in enumerate(result["source_documents"]):
      print(f"[{i+1}] Ø§Ù„Ù…ØµØ¯Ø±: {doc.metadata.get('source', 'Unknown')}")
      print(doc.page_content)
      print("\n-------------------------------------------\n")
    answer = result["result"]
    sources = []
    for doc in result["source_documents"]:
        snippet = doc.page_content[:400]
        src = doc.metadata.get("source", "Unknown")
        sources.append({"source": src, "content": snippet})

    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "result": answer,
            "sources": sources,
            "question": question
        }
    )

if __name__ == "_main_":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    print("ğŸš€ Running on port", port)

    uvicorn.run("main:app", host="0.0.0.0",port=port)
